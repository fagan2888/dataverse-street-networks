{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, zipfile, os, sys, unicodedata, time, datetime, logging as lg\n",
    "import requests\n",
    "from abbrev_state import abbrev_state\n",
    "from keys import api_key\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server = 'https://dataverse.harvard.edu'\n",
    "attempts_max = 3      #how many times to re-try same file upload after error before giving up\n",
    "pause_error = 60     #seconds to pause after an error\n",
    "pause_normal = 10     #seconds to pause between uploads\n",
    "upload_timeout = 300 #how long to set the timeout for upload post requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_doi = 'doi:10.7910/DVN/CUWWYJ' #doi of a dataverse dataset to upload files into\n",
    "manifests = [{\n",
    "                'doi':my_doi, #where to upload to\n",
    "                'folder':'data/usa-cities-graphml', #folder of zip files to upload\n",
    "                'file_desc':'Zip file contains the GraphML files of {}\\'s cities/towns\\' street networks.',\n",
    "                'file_tags':['Data', 'GraphML', 'Cities/Towns']\n",
    "             },\n",
    "             {\n",
    "                'doi':my_doi, \n",
    "                'folder':'data/usa-cities-shapefiles',\n",
    "                'file_desc':'Zip file contains the shapefiles of {}\\'s cities/towns\\' street networks.',\n",
    "                'file_tags':['Data', 'Shapefiles', 'Cities/Towns'] \n",
    "             },\n",
    "             {\n",
    "                'doi':my_doi, \n",
    "                'folder':'data/usa-neighborhoods-graphml',\n",
    "                'file_desc':'Zip file contains the GraphML files of {}\\'s neighborhoods\\' street networks.',\n",
    "                'file_tags':['Data', 'GraphML', 'Neighborhoods']\n",
    "             },\n",
    "             {\n",
    "                'doi':my_doi, \n",
    "                'folder':'data/usa-neighborhoods-shapefiles',\n",
    "                'file_desc':'Zip file contains the shapefiles of {}\\'s neighborhoods\\' street networks.',\n",
    "                'file_tags':['Data', 'Shapefiles', 'Neighborhoods'] \n",
    "             },\n",
    "             {\n",
    "                'doi':my_doi, \n",
    "                'folder':'data/usa-urbanized_areas-graphml',\n",
    "                'file_desc':'Zip file contains the GraphML files of {}\\'s urbanized areas\\' street networks.',\n",
    "                'file_tags':['Data', 'GraphML', 'Urbanized Areas']\n",
    "             },\n",
    "             {\n",
    "                'doi':my_doi, \n",
    "                'folder':'data/usa-urbanized_areas-shapefiles',\n",
    "                'file_desc':'Zip file contains the shapefiles of {}\\'s urbanized areas\\' street networks.',\n",
    "                'file_tags':['Data', 'Shapefiles', 'Urbanized Areas'] \n",
    "             }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(message, level=lg.INFO, name='fp', filename='fp'):\n",
    "\n",
    "    # get the current logger (or create a new one, if none), then log message at requested level\n",
    "    logger = get_logger(level=level, name=name, filename=filename)\n",
    "    if level == lg.DEBUG:\n",
    "        logger.debug(message)\n",
    "    elif level == lg.INFO:\n",
    "        logger.info(message)\n",
    "    elif level == lg.WARNING:\n",
    "        logger.warning(message)\n",
    "    elif level == lg.ERROR:\n",
    "        logger.error(message)\n",
    "    \n",
    "    # print to console\n",
    "    standard_out = sys.stdout\n",
    "    sys.stdout = sys.__stdout__\n",
    "    message = unicodedata.normalize('NFKD', str(message)).encode('ascii', errors='replace').decode()\n",
    "    print(message)\n",
    "    sys.stdout = standard_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logger(level, name, filename, folder='logs'):\n",
    "\n",
    "    logger = lg.getLogger(name)\n",
    "\n",
    "    # if a logger with this name is not already set up\n",
    "    if not getattr(logger, 'handler_set', None):\n",
    "\n",
    "        # get today's date and construct a log filename\n",
    "        todays_date = datetime.datetime.today().strftime('%Y_%m_%d')\n",
    "        log_filename = '{}/{}_{}.log'.format(folder, filename, todays_date)\n",
    "\n",
    "        # if the logs folder does not already exist, create it\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # create file handler and log formatter and set them up\n",
    "        handler = lg.FileHandler(log_filename, encoding='utf-8')\n",
    "        formatter = lg.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(level)\n",
    "        logger.handler_set = True\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zip a file, open it, and return the buffer\n",
    "# this will double-zip the zip files because dataverse unzips zip files when they are uploaded\n",
    "# the result is that dataverse hosts the original zip file\n",
    "def get_file_to_upload(file_path, archive_name, upload_filepath='data/temp_upload.zip'):\n",
    "\n",
    "    zf = zipfile.ZipFile(file=upload_filepath, mode='w')\n",
    "    zf.write(file_path, arcname=archive_name)\n",
    "    zf.close()\n",
    "    \n",
    "    file = {'file' : open(upload_filepath, 'rb')}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the file description and tags that appear on dataverse\n",
    "def get_payload_to_upload(file_desc, file_tags, filename):\n",
    "    \n",
    "    # convert 2-digit state abbreviation to full state name and add it to description and tags\n",
    "    state_abbrev = filename.split('-')[1]\n",
    "    state_name = abbrev_state[state_abbrev]\n",
    "    file_desc = file_desc.format(state_name)\n",
    "    file_tags = file_tags + [state_name]\n",
    "    \n",
    "    params = {'description':file_desc, 'categories':file_tags}\n",
    "    param_str = json.dumps(params)\n",
    "    payload = {'jsonData':param_str}\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upload a file to a dataverse dataset\n",
    "def upload_file(folder, filename, doi, file_desc, file_tags, attempt_count=1):\n",
    "\n",
    "    file_path = '{}/{}'.format(folder, filename)\n",
    "    response = None\n",
    "    \n",
    "    # set up the api endpoint, open the file, and make the payload\n",
    "    endpoint = 'api/datasets/:persistentId/add?persistentId={}&key={}'.format(doi, api_key)\n",
    "    url = '{}/{}'.format(server, endpoint)\n",
    "    file = get_file_to_upload(file_path=file_path, archive_name=filename)\n",
    "    payload = get_payload_to_upload(file_desc=file_desc, file_tags=file_tags, filename=filename)\n",
    "    \n",
    "    try:\n",
    "        # upload the file to the server\n",
    "        log('uploading \"{}\" to {}'.format(filename, doi))\n",
    "        start_time = time.time()\n",
    "        session = requests.Session()\n",
    "        response = session.post(url, data=payload, files=file, timeout=upload_timeout)\n",
    "        log('response {} in {:,.1f} seconds'.format(response.status_code, time.time()-start_time))\n",
    "        \n",
    "        # check if the server response is ok, if not, throw exception\n",
    "        response_json = response.json()\n",
    "        if 'status' in response_json and not response_json['status'] == 'OK':\n",
    "            raise Exception(response_json['message'])\n",
    "        \n",
    "        session.close()\n",
    "        time.sleep(pause_normal)\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        # if any exception is thrown, log it, and retry the upload if we haven't exceeded max number of tries\n",
    "        log(e, level=lg.ERROR)\n",
    "        session.close()\n",
    "        reboot_router()\n",
    "        time.sleep(pause_error)\n",
    "        refresh_ip()\n",
    "        time.sleep(pause_error / 3)\n",
    "        \n",
    "        if attempt_count < attempts_max:\n",
    "            attempt_count += 1\n",
    "            log('re-trying (attempt {} of {})'.format(attempt_count, attempts_max))\n",
    "            response = upload_file(folder, filename, doi, file_desc, file_tags, attempt_count=attempt_count)\n",
    "        else:\n",
    "            log('no more attempts for this file, we give up', level=lg.WARN)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the filenames that currently exist in a dataset\n",
    "def get_previously_uploaded_filenames(dataset_doi):\n",
    "    endpoint = 'api/datasets/:persistentId/versions/:draft/files?key={}&persistentId={}'.format(api_key, dataset_doi)\n",
    "    url = '{}/{}'.format(server, endpoint)\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "    if 'data' in response_json and len(response_json['data']) > 0:\n",
    "        uploaded_files = response_json['data']\n",
    "        uploaded_filenames = [file['dataFile']['filename'] for file in uploaded_files]\n",
    "    else:\n",
    "        uploaded_filenames = []\n",
    "    return uploaded_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refresh_ip():\n",
    "    \n",
    "    commands = ['ipconfig /release',\n",
    "                'ipconfig /flushdns',\n",
    "                'ipconfig /renew']\n",
    "    \n",
    "    for command in commands:\n",
    "        call(command, shell=True)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reboot_router(router_address='192.168.1.1'):\n",
    "    \n",
    "    import telnetlib\n",
    "    from keys import router_username, router_password\n",
    "    \n",
    "    log('rebooting router...')\n",
    "    tn = telnetlib.Telnet(host=router_address, port=23, timeout=5)\n",
    "    tn.read_until(b'login: ')\n",
    "    tn.write(router_username + b'\\r\\n')\n",
    "    tn.read_until(b'Password: ')\n",
    "    tn.write(router_password + b'\\r\\n')\n",
    "    tn.write(b'reboot\\r\\n')\n",
    "    tn.write(b'exit\\r\\n')\n",
    "    display = tn.read_all()\n",
    "    tn.close()\n",
    "    return display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('script started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manifest in manifests:\n",
    "    \n",
    "    log('manifest={}'.format(manifest))\n",
    "    \n",
    "    # get list of filenames in this folder to upload\n",
    "    folder = manifest['folder']\n",
    "    file_desc = manifest['file_desc']\n",
    "    file_tags = manifest['file_tags']\n",
    "    doi = manifest['doi']\n",
    "    already_uploaded = get_previously_uploaded_filenames(doi)\n",
    "    \n",
    "    # upload each file\n",
    "    filenames = os.listdir(folder)\n",
    "    for filename in filenames:\n",
    "        if not filename in already_uploaded:\n",
    "            # if it's not already on the server, upload it\n",
    "            response = upload_file(folder, filename, doi, file_desc, file_tags)\n",
    "        else:\n",
    "            log('skipping \"{}\" because it is already on the server'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('script finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
